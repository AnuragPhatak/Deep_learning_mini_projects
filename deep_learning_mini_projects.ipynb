{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ebd9d92-5c46-4a7b-a50e-065a0c183683",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import r2_score\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1838b75a-b60e-4ce1-8442-39bac3a5a265",
   "metadata": {},
   "source": [
    "### **Predicting Concrete Compressive Strength (MPa)**\n",
    "\n",
    "Build an Artificial Neural Network (ANN) to accurately predict the compressive strength of concrete (measured in MPa) based on the given mixture composition and the specific age of the concrete (in days) using the provided dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd6f77f-695e-49eb-853e-182c5fdfb172",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('concrete_data.csv')\n",
    "df.drop_duplicates(inplace=True)\n",
    "\n",
    "\n",
    "X = df.drop(columns=['Strength'])  \n",
    "y = df['Strength']              \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "model = Sequential(name=\"Concrete_Strength_ANN\")\n",
    "model.add(Dense(64, activation='relu', input_dim=X_train_scaled.shape[1])) \n",
    "model.add(Dense(32, activation='relu')) \n",
    "model.add(Dense(32, activation='relu')) \n",
    "model.add(Dense(1, activation='linear'))\n",
    "model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])\n",
    "history = model.fit(X_train_scaled, y_train, epochs=200, batch_size=32, validation_split=0.2, verbose=1)\n",
    "loss, mae = model.evaluate(X_test_scaled, y_test, verbose=1)\n",
    "print(f\"Test Loss: {loss}\")\n",
    "print(f\"Test MAE: {mae}\")\n",
    "\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(f\"R² Score: {r2}\")\n",
    "\n",
    "\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(history.history['mae'], label='Training MAE')\n",
    "plt.plot(history.history['val_mae'], label='Validation MAE')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('MAE')\n",
    "plt.legend()\n",
    "plt.title('Training and Validation MAE')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f774a75d-2a27-4faa-b8aa-64b0f7ff0b84",
   "metadata": {},
   "source": [
    "# hyperparameter tunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25aa7c55-fa80-44cf-be95-dfaf68c8952c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from  tensorflow.keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "def create_model(learning_rate, dropout_rate, activation_function, init, neuron1, neuron2):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(neuron1, input_dim=X_train_scaled.shape[1], kernel_initializer=init, activation=activation_function))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(neuron2, kernel_initializer=init, activation=activation_function))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(1, activation='linear'))  # Output layer for regression\n",
    "\n",
    "    adam = Adam(learning_rate=learning_rate)\n",
    "    model.compile(loss='mean_squared_error', optimizer=adam, metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "model = KerasRegressor(build_fn=create_model, verbose=0)\n",
    "\n",
    "batch_size = [16, 32, 64]\n",
    "epochs = [100, 200]\n",
    "learning_rate = [0.001, 0.01, 0.1]\n",
    "dropout_rate = [0.2, 0.3, 0.5]\n",
    "activation_function = ['relu', 'tanh', 'linear']\n",
    "init = ['uniform', 'normal']\n",
    "neuron1 = [32, 64, 128]\n",
    "neuron2 = [16, 32, 64]\n",
    "\n",
    "param_grids = dict(\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    learning_rate=learning_rate,\n",
    "    dropout_rate=dropout_rate,\n",
    "    activation_function=activation_function,\n",
    "    init=init,\n",
    "    neuron1=neuron1,\n",
    "    neuron2=neuron2\n",
    ")\n",
    "\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grids, cv=KFold(), verbose=10)\n",
    "grid_result = grid.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(f'Best: {grid_result.best_score_} using {grid_result.best_params_}')\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(f'Mean: {mean:.4f}, StdDev: {stdev:.4f}, Params: {param}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a0ffd1-fd25-4e90-bfa1-b532fb3f5ac2",
   "metadata": {},
   "source": [
    "### Explanation**\n",
    "\n",
    "This code builds, trains, evaluates, and fine-tunes an Artificial Neural Network (ANN) to predict concrete strength based on input features. The workflow involves the following steps:\n",
    "\n",
    "1. **Data Preparation**: \n",
    "   - The dataset is loaded, and duplicate rows are removed to ensure data quality. Features (`X`) and target (`y`) variables are separated, and the dataset is split into training and testing subsets (80-20 split). Feature scaling is applied using `StandardScaler` to standardize the input features, improving model performance and training stability.\n",
    "\n",
    "2. **Model Definition**:\n",
    "   - A sequential model is constructed with three hidden layers using the `relu` activation function for non-linear transformations. The final output layer uses a `linear` activation for regression. The model is compiled with the Adam optimizer, `mean_squared_error` as the loss function, and `mae` (Mean Absolute Error) as a performance metric.\n",
    "\n",
    "3. **Model Training**:\n",
    "   - The model is trained for 200 epochs with a batch size of 32. A validation split of 20% ensures performance is monitored on unseen data during training. The training history is stored for visualization.\n",
    "\n",
    "4. **Evaluation and Predictions**:\n",
    "   - The model's performance is evaluated on the test dataset using loss (`mean_squared_error`), MAE, and R² score to assess predictive accuracy. Predicted values (`y_pred`) are compared against actual test values.\n",
    "\n",
    "5. **Performance Visualization**:\n",
    "   - Training and validation losses, along with Mean Absolute Errors (MAE), are plotted across epochs to observe the model's convergence and detect signs of overfitting or underfitting.\n",
    "\n",
    "6. **Hyperparameter Tuning**:\n",
    "   - A dynamic model creation function is defined, allowing flexible specification of parameters like learning rate, dropout rate, activation functions, weight initializers, and the number of neurons in each layer. \n",
    "   - Using `GridSearchCV` with `KFold` cross-validation, the model is fine-tuned by testing multiple combinations of hyperparameters to find the optimal configuration. Results include the best model and detailed scores for all parameter combinations.\n",
    "\n",
    "This comprehensive approach ensures robust model training, evaluation, and optimization, providing insights into both the model's strengths and potential areas for improvement.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6077f82-5fa7-4493-abe7-61290ec60e02",
   "metadata": {},
   "source": [
    "### **Fashion MNIST Classification Project**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed0eee86-b3a0-4348-a67d-0856743123f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = keras.datasets.fashion_mnist.load_data()\n",
    "\n",
    "class_labels = [\"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\",\n",
    "                \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"]\n",
    "\n",
    "plt.figure(figsize=(16, 16))\n",
    "for i in range(25):\n",
    "    plt.subplot(5, 5, i + 1)\n",
    "    plt.imshow(X_train[i], cmap=\"Greys\")\n",
    "    plt.axis('off')\n",
    "    plt.title(f\"{class_labels[y_train[i]]} ({y_train[i]})\", fontsize=10)\n",
    "plt.show()\n",
    "\n",
    "X_train = X_train / 255.0\n",
    "X_test = X_test / 255.0\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dense(units=32, activation='relu'),\n",
    "    keras.layers.Dense(units=10, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(X_train, y_train, epochs=10, verbose=1)\n",
    "\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Loss: {test_loss}, Test Accuracy: {test_accuracy}\")\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "plt.figure(figsize=(16, 16))\n",
    "for i in range(25):\n",
    "    plt.subplot(5, 5, i + 1)\n",
    "    plt.imshow(X_test[i], cmap=\"Greys\")\n",
    "    plt.axis('off')\n",
    "    plt.title(f\"Actual: {class_labels[y_test[i]]}\\nPredicted: {class_labels[np.argmax(y_pred[i])]}\")\n",
    "plt.show()\n",
    "\n",
    "cm = confusion_matrix(y_test, [np.argmax(pred) for pred in y_pred])\n",
    "plt.figure(figsize=(16, 9))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"coolwarm\", xticklabels=class_labels, yticklabels=class_labels)\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.show()\n",
    "\n",
    "cr = classification_report(y_test, [np.argmax(pred) for pred in y_pred], target_names=class_labels)\n",
    "print(\"Classification Report:\\n\", cr)\n",
    "\n",
    "model.save(\"MNIST_classifier_nn_model.h5\")\n",
    "loaded_model = keras.models.load_model(\"MNIST_classifier_nn_model.h5\")\n",
    "\n",
    "loaded_pred = loaded_model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c46c12e2-d557-40d3-8723-7dde26e4c1f0",
   "metadata": {},
   "source": [
    "### **Explanation**\n",
    "\n",
    "This project demonstrates the use of a simple Artificial Neural Network (ANN) to classify images from the Fashion MNIST dataset. The dataset contains grayscale images of 28x28 pixels, each representing one of 10 fashion categories (e.g., T-shirt, dress, or sneakers). Below is the workflow:\n",
    "\n",
    "1. **Importing Libraries**:\n",
    "   - Essential libraries like Keras, NumPy, Matplotlib, and Seaborn are imported for model building, data manipulation, and visualization.\n",
    "   \n",
    "2. **Dataset Loading**:\n",
    "   - The Fashion MNIST dataset is loaded, split into training (60,000 images) and testing (10,000 images) sets.\n",
    "   - Labels are mapped to class names like T-shirt, Trouser, Dress, etc., for interpretability.\n",
    "\n",
    "3. **Data Visualization**:\n",
    "   - Sample images from the training set are visualized along with their labels to understand the data distribution and format.\n",
    "\n",
    "4. **Feature Scaling**:\n",
    "   - Pixel values are normalized to a range of 0 to 1 by dividing them by 255. This scaling improves the convergence of the neural network during training.\n",
    "\n",
    "5. **Model Construction**:\n",
    "   - A simple ANN is built using Keras Sequential API:\n",
    "     - **Flatten Layer**: Converts the 2D image input into a 1D array.\n",
    "     - **Hidden Layer**: A dense layer with 32 neurons and ReLU activation for learning non-linear patterns.\n",
    "     - **Output Layer**: A dense layer with 10 neurons (one for each class) and a softmax activation function for multi-class classification.\n",
    "\n",
    "6. **Model Compilation**:\n",
    "   - The model uses:\n",
    "     - **Adam Optimizer**: For adaptive learning.\n",
    "     - **Sparse Categorical Crossentropy Loss**: For multi-class classification.\n",
    "     - **Accuracy Metric**: To evaluate model performance.\n",
    "\n",
    "7. **Model Training**:\n",
    "   - The model is trained on the normalized training dataset for 10 epochs to optimize weights and minimize loss.\n",
    "\n",
    "8. **Model Evaluation**:\n",
    "   - The trained model is evaluated on the test dataset, and metrics like loss and accuracy are reported.\n",
    "\n",
    "9. **Prediction and Visualization**:\n",
    "   - Predictions are made on the test dataset, and the results are visualized by comparing actual labels with predicted labels for a subset of images.\n",
    "\n",
    "10. **Confusion Matrix**:\n",
    "    - A confusion matrix is generated and visualized as a heatmap to understand the model's performance across all classes.\n",
    "\n",
    "11. **Classification Report**:\n",
    "    - Precision, recall, and F1-score for each class are displayed to provide a detailed analysis of the model's strengths and weaknesses.\n",
    "\n",
    "12. **Model Saving and Reloading**:\n",
    "    - The trained model is saved as a `.h5` file for future use and reloaded to verify its predictions on the test data.\n",
    "\n",
    "This project highlights the key steps in building and deploying a neural network for image classification tasks while providing detailed performance insights through evaluation metrics.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
